{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFWs3xZuBjCs",
        "outputId": "49d3eeec-3dff-47d8-c519-e2d0713a057d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language: en\n",
            "Word count: 19\n",
            "Sentence count: 2\n",
            "Tokens: [' ', 'Hi', 'there', '!', 'I', 'am', 'Pratham', 'Kubetkar', 'a', 'student', 'of', 'TYBTech', 'CSE', '(', 'AI&ML', ')', 'from', 'PCCOE', 'pune']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# text =\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the News category dataset\n",
        "news_data =\" Hi there ! I am  a student of TYBTech CSE( AI&ML ) from PCCOE pune\"\n",
        "\n",
        "# Function to perform language detection using Spacy\n",
        "def detect_language(text):\n",
        "    doc = nlp(text)\n",
        "    return doc.lang_\n",
        "\n",
        "# Function to perform word count\n",
        "def count_words(text):\n",
        "    doc = nlp(text)\n",
        "    return len(doc)\n",
        "\n",
        "# Function to perform sentence count\n",
        "def count_sentences(text):\n",
        "    doc = nlp(text)\n",
        "    return len(list(doc.sents))\n",
        "\n",
        "# Function to perform word-level tokenization\n",
        "def word_tokenization(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Example usage:\n",
        "sample_text = news_data  # Taking the headline of the first news article as an example\n",
        "\n",
        "# Language detection\n",
        "language = detect_language(\"Bonjour\")\n",
        "print(\"Language:\", language)\n",
        "\n",
        "# Word count\n",
        "word_count = count_words(sample_text)\n",
        "print(\"Word count:\", word_count)\n",
        "\n",
        "# Sentence count\n",
        "sentence_count = count_sentences(sample_text)\n",
        "print(\"Sentence count:\", sentence_count)\n",
        "\n",
        "# Word-level tokenization\n",
        "tokens = word_tokenization(sample_text)\n",
        "print(\"Tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZtykV59Bj-C",
        "outputId": "3fab6ed0-a739-49e8-b74d-1371a0995dcd"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the News category dataset\n",
        "news_data =\" Hi there ! I am a student of TYBTech CSE( AI&ML ) from PCCOE pune\"\n",
        "\n",
        "# Initialize the Porter stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Function to perform stemming on a text\n",
        "def perform_stemming(text):\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "    # Apply stemming to each word\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "    # Join the stemmed words back into a single string\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    return stemmed_text\n",
        "\n",
        "# Example usage:\n",
        "sample_text = news_data  # Taking the headline of the first news article as an example\n",
        "\n",
        "# Perform stemming on the sample text\n",
        "stemmed_text = perform_stemming(sample_text)\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nStemmed text:\")\n",
        "print(stemmed_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oqwmYH7DlcR",
        "outputId": "832e9534-5a0d-46b6-b32c-9281320a18c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS Tags:\n",
            "[('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('bird', 'NN'), ('hand', 'NN'), ('worth', 'IN'), ('two', 'CD'), ('bush', 'NN'), ('.', '.')]\n",
            "\n",
            "Frequency List of POS Tags:\n",
            "Counter({'NN': 6, 'JJ': 2, '.': 2, 'NNS': 1, 'IN': 1, 'CD': 1})\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to perform POS tagging and generate frequency list of POS tags\n",
        "def pos_tagging(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Filter out stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Perform POS tagging\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "    # Generate frequency list of POS tags\n",
        "    pos_freq = Counter(tag for word, tag in pos_tags)\n",
        "\n",
        "    return pos_tags, pos_freq\n",
        "\n",
        "# Example usage:\n",
        "sample_text = \"\"\"The quick brown fox jumps over the lazy dog.\n",
        "                 A bird in the hand is worth two in the bush.\"\"\"\n",
        "\n",
        "# Perform POS tagging and generate frequency list of POS tags\n",
        "pos_tags, pos_freq = pos_tagging(sample_text)\n",
        "\n",
        "# Print POS tags and their frequencies\n",
        "print(\"POS Tags:\")\n",
        "print(pos_tags)\n",
        "print(\"\\nFrequency List of POS Tags:\")\n",
        "print(pos_freq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvV98V__I88P",
        "outputId": "500f042c-c07a-4f85-ec8e-5320cd0b7990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS Tags:\n",
            "The DET\n",
            "quick ADJ\n",
            "brown ADJ\n",
            "fox NOUN\n",
            "jumps VERB\n",
            "over ADP\n",
            "the DET\n",
            "lazy ADJ\n",
            "dog NOUN\n",
            ". PUNCT\n",
            "\n",
            "                  SPACE\n",
            "A DET\n",
            "bird NOUN\n",
            "in ADP\n",
            "the DET\n",
            "hand NOUN\n",
            "is AUX\n",
            "worth ADJ\n",
            "two NUM\n",
            "in ADP\n",
            "the DET\n",
            "bush NOUN\n",
            ". PUNCT\n",
            "\n",
            "Frequency List of POS Tags:\n",
            "Counter({'DET': 5, 'NOUN': 5, 'ADJ': 4, 'ADP': 3, 'PUNCT': 2, 'VERB': 1, 'SPACE': 1, 'AUX': 1, 'NUM': 1})\n"
          ]
        }
      ],
      "source": [
        "                                                                     # STATEMENT 3 USING SPACY\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform POS tagging and generate frequency list of POS tags\n",
        "def pos_tagging_spacy(text):\n",
        "    # Process the text using SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Generate frequency list of POS tags\n",
        "    pos_freq = Counter(token.pos_ for token in doc)\n",
        "\n",
        "    return doc, pos_freq\n",
        "\n",
        "# Example usage:\n",
        "sample_text = \"\"\"The quick brown fox jumps over the lazy dog.\n",
        "                 A bird in the hand is worth two in the bush.\"\"\"\n",
        "\n",
        "# Perform POS tagging and generate frequency list of POS tags using SpaCy\n",
        "doc, pos_freq = pos_tagging_spacy(sample_text)\n",
        "\n",
        "# Print POS tags and their frequencies\n",
        "print(\"POS Tags:\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n",
        "\n",
        "print(\"\\nFrequency List of POS Tags:\")\n",
        "print(pos_freq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gSHhw4PM590",
        "outputId": "624368d4-bab0-4c21-9aa9-21837cc490ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment: Positive\n",
            "Emotions: happy\n"
          ]
        }
      ],
      "source": [
        "#STATEMENT 4\n",
        "\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "\n",
        "# Function to identify sentiment (positive, negative, neutral)\n",
        "def identify_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "    if sentiment_score > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment_score < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Function to identify emotions expressed in text\n",
        "def identify_emotions(text):\n",
        "    emotions = {\n",
        "        'happy': ['happy', 'joy', 'excited', 'delighted', 'glad'],\n",
        "        'sad': ['sad', 'unhappy', 'grief', 'depressed'],\n",
        "        'angry': ['angry', 'mad', 'irritated', 'frustrated', 'enraged'],\n",
        "        # Add more emotions as needed\n",
        "    }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    emotion_counter = Counter()\n",
        "\n",
        "    for emotion, keywords in emotions.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword in text_lower:\n",
        "                emotion_counter[emotion] += 1\n",
        "\n",
        "    if emotion_counter:\n",
        "        dominant_emotion = emotion_counter.most_common(1)[0][0]\n",
        "        return dominant_emotion\n",
        "    else:\n",
        "        return 'No Emotion Detected'\n",
        "\n",
        "# Example usage:\n",
        "sample_text = \"I'm so happy and excited about this wonderful news!\"\n",
        "\n",
        "# Identify sentiment\n",
        "sentiment = identify_sentiment(sample_text)\n",
        "print(\"Sentiment:\", sentiment)\n",
        "\n",
        "# Identify emotions\n",
        "emotions = identify_emotions(sample_text)\n",
        "print(\"Emotions:\", emotions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV1xucBXNO1x"
      },
      "outputs": [],
      "source": [
        "#STATEMENT 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxXEny-cPN6S",
        "outputId": "63a50cfb-f39c-4b92-8526-3dedc841e7f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability of sequence 'the quick brown fox jumps': 0.5\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class NgramLanguageModel:\n",
        "    def __init__(self, corpus, n):\n",
        "        self.n = n\n",
        "        self.ngram_counts = defaultdict(int)\n",
        "        self.context_counts = defaultdict(int)\n",
        "        self.vocab = set()\n",
        "\n",
        "        # Train the language model on the given corpus\n",
        "        self.train(corpus)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        # Tokenize the corpus into words\n",
        "        words = corpus.split()\n",
        "        # Iterate over each n-gram in the corpus\n",
        "        for i in range(len(words) - self.n + 1):\n",
        "            ngram = tuple(words[i:i+self.n])\n",
        "            context = ngram[:-1]\n",
        "            word = ngram[-1]\n",
        "            # Update counts\n",
        "            self.ngram_counts[ngram] += 1\n",
        "            self.context_counts[context] += 1\n",
        "            self.vocab.add(word)\n",
        "\n",
        "    def probability(self, sequence):\n",
        "        # Tokenize the sequence into words\n",
        "        words = sequence.split()\n",
        "        # Initialize probability\n",
        "        prob = 1.0\n",
        "        # Calculate probability using N-gram model\n",
        "        for i in range(len(words) - self.n + 1):\n",
        "            ngram = tuple(words[i:i+self.n])\n",
        "            context = ngram[:-1]\n",
        "            word = ngram[-1]\n",
        "            # Calculate conditional probability\n",
        "            if context in self.context_counts and ngram in self.ngram_counts:\n",
        "                prob *= self.ngram_counts[ngram] / self.context_counts[context]\n",
        "            else:\n",
        "                # Laplace smoothing for unseen n-grams\n",
        "                prob *= 1 / (len(self.vocab) + self.context_counts[context])\n",
        "        return prob\n",
        "\n",
        "# Example usage:\n",
        "corpus = \"the quick brown fox jumps over the lazy dog\"\n",
        "n = 2  # Change the value of n for different N-gram models\n",
        "\n",
        "# Initialize and train the N-gram language model\n",
        "ngram_model = NgramLanguageModel(corpus, n)\n",
        "\n",
        "# Determine the probability of a sequence of words\n",
        "sequence = \"the quick brown fox jumps\"\n",
        "probability = ngram_model.probability(sequence)\n",
        "\n",
        "print(\"Probability of sequence '{}': {}\".format(sequence, probability))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bd3qhclPO6J"
      },
      "outputs": [],
      "source": [
        "# STATEMENT 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "sS_tLRSuPgjS",
        "outputId": "9ca7e39f-f07a-44e3-8ac4-ebe1b073b1da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple Inc.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is an \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " multinational technology company headquartered in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Cupertino\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    California\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".<br>                 It was founded by \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Steve Jobs\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Steve Wozniak\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ronald Wayne\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1976\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Named Entities:\n",
            "Apple Inc.: ORG\n",
            "American: NORP\n",
            "Cupertino: GPE\n",
            "California: GPE\n",
            "Steve Jobs: PERSON\n",
            "Steve Wozniak: PERSON\n",
            "Ronald Wayne: PERSON\n",
            "1976: DATE\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load the SpaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform Named Entity Recognition and visualize named entities\n",
        "def ner_and_visualize(text):\n",
        "    # Process the text using SpaCy NER\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Visualize named entities\n",
        "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
        "\n",
        "    # Extract named entities and their labels\n",
        "    named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "\n",
        "    return named_entities\n",
        "\n",
        "# Example usage:\n",
        "sample_text = \"\"\"Apple Inc. is an American multinational technology company headquartered in Cupertino, California.\n",
        "                 It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976.\"\"\"\n",
        "\n",
        "# Perform Named Entity Recognition and visualize named entities\n",
        "named_entities = ner_and_visualize(sample_text)\n",
        "\n",
        "# Print named entities and their labels\n",
        "print(\"Named Entities:\")\n",
        "for entity, label in named_entities:\n",
        "    print(f\"{entity}: {label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0XjgjSWPwWS",
        "outputId": "8d915b24-c152-4498-dc1c-679d6e9f7a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: This movie is amazing! I loved every moment of it.\n",
            "Sentiment: Positive\n",
            "\n",
            "Review: The acting was terrible, and the plot made no sense.\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: It was an okay movie, nothing special.\n",
            "Sentiment: Positive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Develop a text classification model for movie review analysis by implementing suitable\n",
        "# NLP pipeline\n",
        "\n",
        "# Similar to sentiment analysis\n",
        "\n",
        "\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    # Stop word removal (assuming stopwords are already imported)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Punctuation removal\n",
        "    tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function for text classification using TextBlob\n",
        "def classify_text(text):\n",
        "    # Preprocess the text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Perform sentiment analysis using TextBlob\n",
        "    blob = TextBlob(preprocessed_text)\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "\n",
        "    # Classify the sentiment\n",
        "    if sentiment_score > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment_score < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Example usage:\n",
        "sample_reviews = [\n",
        "    \"This movie is amazing! I loved every moment of it.\",\n",
        "    \"The acting was terrible, and the plot made no sense.\",\n",
        "    \"It was an okay movie, nothing special.\",\n",
        "]\n",
        "\n",
        "for review in sample_reviews:\n",
        "    sentiment = classify_text(review)\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Sentiment: {sentiment}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "tQI498NOQR56",
        "outputId": "40484a45-6bbf-4674-deae-7a93e797b079"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'spam_ham_dataset.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e7a29a16e2a6>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spam_ham_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Data Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spam_ham_dataset.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"spam_ham_dataset.csv\")\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove non-alphanumeric characters and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
        "    return text\n",
        "\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(data['text'])\n",
        "y = data['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Training\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Example usage\n",
        "def classify_message(message):\n",
        "    message = preprocess_text(message)\n",
        "    message_vectorized = vectorizer.transform([message])\n",
        "    prediction = model.predict(message_vectorized)[0]\n",
        "    return prediction\n",
        "\n",
        "# Example usage\n",
        "sample_messages = [\n",
        "    \"Congratulations! You've won a free trip to Hawaii. Click here to claim your prize.\",\n",
        "    \"Hi John, can you send me the report by tomorrow?\",\n",
        "    \"URGENT: Your account has been compromised. Please reset your password immediately.\",\n",
        "]\n",
        "\n",
        "for message in sample_messages:\n",
        "    prediction = classify_message(message)\n",
        "    print(f\"Message: {message}\")\n",
        "    print(f\"Prediction: {prediction}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "GQu8ZOD8UQVj",
        "outputId": "2eceb8d2-ce23-4e6f-f54a-2a9ec9f4ec3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93muniversal_tagset\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('universal_tagset')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/universal_tagset/en-brown.map\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7cfecf33f24c>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Split the data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagged_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagged_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pieces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;31m# Iterate to the end of the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# Get everything we can from this piece.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tok\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# Update the offset table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m                 \u001b[0;34m\"block reader %s() should return list or tuple.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/tagged.py\u001b[0m in \u001b[0;36mread_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 ]\n\u001b[1;32m    297\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag_mapping_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag_mapping_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/tagged.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 ]\n\u001b[1;32m    297\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag_mapping_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag_mapping_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/tagged.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \"\"\"\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtagset\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mtag_mapping_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mtag_mapping_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/mapping.py\u001b[0m in \u001b[0;36mmap_tag\u001b[0;34m(source, target, source_tag)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"en-brown\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagset_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_tag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/mapping.py\u001b[0m in \u001b[0;36mtagset_mapping\u001b[0;34m(source, target)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPPINGS\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPPINGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"universal\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0m_load_universal_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Added the new Russian National Corpus mappings because the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Russian model for nltk.pos_tag() uses it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/mapping.py\u001b[0m in \u001b[0;36m_load_universal_map\u001b[0;34m(fileid)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_universal_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_UNIVERSAL_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# When mapping to the Universal Tagset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93muniversal_tagset\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('universal_tagset')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/universal_tagset/en-brown.map\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import hmm\n",
        "\n",
        "# Download the Brown corpus\n",
        "nltk.download('brown')\n",
        "\n",
        "# Prepare training data\n",
        "tagged_sentences = brown.tagged_sents(tagset='universal')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = tagged_sentences[:int(0.8 * len(tagged_sentences))]\n",
        "test_data = tagged_sentences[int(0.8 * len(tagged_sentences)):]\n",
        "\n",
        "# Train the HMM model\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train_supervised(train_data)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "accuracy = tagger.evaluate(test_data)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Test the model on a sample sentence\n",
        "sample_sentence = \"This is a sample sentence.\"\n",
        "tokenized_sentence = nltk.word_tokenize(sample_sentence)\n",
        "predicted_tags = tagger.tag(tokenized_sentence)\n",
        "print(\"Predicted POS tags:\", predicted_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esg__m1_UkRr",
        "outputId": "80e69710-5f5a-4acb-efe5-970a41655f75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word: ., Embedding: [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
            " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
            " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
            " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
            "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
            "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
            "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
            " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
            "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
            "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
            " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
            " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
            "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
            " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
            "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
            " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
            " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
            " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
            " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
            "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
            " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
            " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
            " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
            "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
            " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
            "Word: pizza, Embedding: [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
            "Word: friends, Embedding: [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
            "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
            " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
            "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
            "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
            " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
            "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
            " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
            " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
            " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
            " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
            "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
            " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
            " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
            " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
            "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
            " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
            " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
            "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
            "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
            "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
            "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
            "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
            "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
            "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
            "Word: i, Embedding: [-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
            "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
            "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
            "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
            "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
            "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
            " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
            "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
            " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
            "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
            "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
            "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
            " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
            " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
            "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
            "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
            "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
            " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
            " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
            " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
            " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
            " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
            " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
            " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
            "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n",
            "Word: important, Embedding: [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
            "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
            " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
            "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
            " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
            "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
            " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
            " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
            "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
            " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
            "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
            "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
            " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
            "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
            " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
            "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
            "  0.00620989  0.00481889  0.00078719  0.00301345]\n",
            "Word: are, Embedding: [-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
            " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
            " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
            " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
            " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
            " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
            " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
            "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
            " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
            "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
            "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
            " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
            " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
            "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
            "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
            "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
            "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
            " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
            "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
            " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
            "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
            " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
            " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
            " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
            " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n",
            "Word: with, Embedding: [ 8.1323227e-03 -4.4573620e-03 -1.0683639e-03  1.0063711e-03\n",
            " -1.9111515e-04  1.1481846e-03  6.1138989e-03 -2.0271667e-05\n",
            " -3.2459856e-03 -1.5107380e-03  5.8973357e-03  1.5141116e-03\n",
            " -7.2426652e-04  9.3333060e-03 -4.9213143e-03 -8.3841488e-04\n",
            "  9.1754692e-03  6.7494698e-03  1.5028655e-03 -8.8826166e-03\n",
            "  1.1487532e-03 -2.2882700e-03  9.3682958e-03  1.2099354e-03\n",
            "  1.4900729e-03  2.4064251e-03 -1.8360182e-03 -4.9996651e-03\n",
            "  2.3243096e-04 -2.0141930e-03  6.6009746e-03  8.9401789e-03\n",
            " -6.7475863e-04  2.9770334e-03 -6.1076926e-03  1.6993354e-03\n",
            " -6.9262758e-03 -8.6940806e-03 -5.9002396e-03 -8.9565311e-03\n",
            "  7.2776405e-03 -5.7720677e-03  8.2764039e-03 -7.2435904e-03\n",
            "  3.4216964e-03  9.6750595e-03 -7.7854968e-03 -9.9451197e-03\n",
            " -4.3291734e-03 -2.6831473e-03 -2.7129104e-04 -8.8316062e-03\n",
            " -8.6176116e-03  2.8002281e-03 -8.2064578e-03 -9.0693934e-03\n",
            " -2.3404805e-03 -8.6318618e-03 -7.0566940e-03 -8.4012030e-03\n",
            " -3.0133079e-04 -4.5643267e-03  6.6272160e-03  1.5271700e-03\n",
            " -3.3414967e-03  6.1090100e-03 -6.0133226e-03 -4.6561989e-03\n",
            " -7.2075543e-03 -4.3366072e-03 -1.8093413e-03  6.4896834e-03\n",
            " -2.7704102e-03  4.9189981e-03  6.9044856e-03 -7.4637523e-03\n",
            "  4.5648785e-03  6.1270166e-03 -2.9544930e-03  6.6250633e-03\n",
            "  6.1259177e-03 -6.4435257e-03 -6.7645940e-03  2.5389746e-03\n",
            " -1.6238290e-03 -6.0651661e-03  9.4992686e-03 -5.1301788e-03\n",
            " -6.5541379e-03 -1.1988595e-04 -2.7014450e-03  4.4440309e-04\n",
            " -3.5374803e-03 -4.1933323e-04 -7.0862018e-04  8.2282576e-04\n",
            "  8.1948685e-03 -5.7367431e-03 -1.6595384e-03  5.5716424e-03]\n",
            "Word: eating, Embedding: [ 8.1682196e-03 -4.4430876e-03  8.9854207e-03  8.2536768e-03\n",
            " -4.4352245e-03  3.0311922e-04  4.2745662e-03 -3.9263205e-03\n",
            " -5.5600055e-03 -6.5123411e-03 -6.7066576e-04 -2.9590298e-04\n",
            "  4.4630761e-03 -2.4739392e-03 -1.7266956e-04  2.4618655e-03\n",
            "  4.8677116e-03 -3.0725500e-05 -6.3393908e-03 -9.2609162e-03\n",
            "  2.6671698e-05  6.6618663e-03  1.4661378e-03 -8.9665074e-03\n",
            " -7.9385862e-03  6.5519321e-03 -3.7857031e-03  6.2549310e-03\n",
            " -6.6810292e-03  8.4796371e-03 -6.5162433e-03  3.2881298e-03\n",
            " -1.0569941e-03 -6.7874910e-03 -3.2876716e-03 -1.1613911e-03\n",
            " -5.4710251e-03 -1.2114544e-03 -7.5633861e-03  2.6465494e-03\n",
            "  9.0702381e-03 -2.3773212e-03 -9.7640837e-04  3.5134726e-03\n",
            "  8.6651295e-03 -5.9217340e-03 -6.8876734e-03 -2.9331071e-03\n",
            "  9.1476431e-03  8.6623465e-04 -8.6784046e-03 -1.4470875e-03\n",
            "  9.4793597e-03 -7.5494531e-03 -5.3581996e-03  9.3164509e-03\n",
            " -8.9737549e-03  3.8258014e-03  6.6535384e-04  6.6605979e-03\n",
            "  8.3127497e-03 -2.8508413e-03 -3.9922316e-03  8.8979360e-03\n",
            "  2.0896050e-03  6.2490166e-03 -9.4457883e-03  9.5900670e-03\n",
            " -1.3483969e-03 -6.0521681e-03  2.9925122e-03 -4.5653118e-04\n",
            "  4.7064587e-03 -2.2829606e-03 -4.1377577e-03  2.2778071e-03\n",
            "  8.3544394e-03 -4.9955305e-03  2.6686424e-03 -7.9904739e-03\n",
            " -6.7732711e-03 -4.6774797e-04 -8.7678107e-03  2.7894690e-03\n",
            "  1.5985755e-03 -2.3197669e-03  5.0039077e-03  9.7487234e-03\n",
            "  8.4541878e-03 -1.8802264e-03  2.0581186e-03 -4.0036836e-03\n",
            " -8.2414495e-03  6.2779505e-03 -1.9491903e-03 -6.6619454e-04\n",
            " -1.7712313e-03 -4.5357361e-03  4.0616891e-03 -4.2701121e-03]\n",
            "Word: enjoy, Embedding: [-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n",
            "  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n",
            " -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
            "  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n",
            " -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n",
            "  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n",
            "  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n",
            "  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n",
            "  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n",
            " -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n",
            " -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n",
            " -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n",
            " -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n",
            " -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
            "  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n",
            " -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n",
            " -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n",
            " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n",
            "  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n",
            " -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n",
            " -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n",
            "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n",
            " -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n",
            "  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n",
            "  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n",
            "Word: delicious, Embedding: [-0.00515624 -0.00666834 -0.00777684  0.00831073 -0.00198234 -0.00685496\n",
            " -0.00415439  0.00514413 -0.00286914 -0.00374966  0.00162143 -0.00277629\n",
            " -0.00158436  0.00107449 -0.00297794  0.00851928  0.00391094 -0.00995886\n",
            "  0.0062596  -0.00675425  0.00076943  0.00440423 -0.00510337 -0.00211067\n",
            "  0.00809548 -0.00424379 -0.00763626  0.00925791 -0.0021555  -0.00471943\n",
            "  0.0085708   0.00428334  0.00432484  0.00928451 -0.00845308  0.00525532\n",
            "  0.00203935  0.00418828  0.0016979   0.00446413  0.00448629  0.00610452\n",
            " -0.0032021  -0.00457573 -0.00042652  0.00253373 -0.00326317  0.00605772\n",
            "  0.00415413  0.00776459  0.00256927  0.00811668 -0.00138721  0.00807793\n",
            "  0.00371702 -0.00804732 -0.00393361 -0.00247188  0.00489304 -0.00087216\n",
            " -0.00283091  0.00783371  0.0093229  -0.00161493 -0.00515925 -0.00470176\n",
            " -0.00484605 -0.00960283  0.00137202 -0.00422492  0.00252671  0.00561448\n",
            " -0.00406591 -0.00959658  0.0015467  -0.00670012  0.00249517 -0.00378063\n",
            "  0.00707842  0.00064022  0.00356094 -0.00273913 -0.00171055  0.00765279\n",
            "  0.00140768 -0.00585045 -0.0078345   0.00123269  0.00645463  0.00555635\n",
            " -0.00897705  0.00859216  0.00404698  0.00746961  0.00974633 -0.00728958\n",
            " -0.00903996  0.005836    0.00939121  0.00350693]\n",
            "Word: is, Embedding: [ 7.0887972e-03 -1.5679300e-03  7.9474989e-03 -9.4886590e-03\n",
            " -8.0294991e-03 -6.6403709e-03 -4.0034545e-03  4.9892161e-03\n",
            " -3.8135587e-03 -8.3199050e-03  8.4117772e-03 -3.7470020e-03\n",
            "  8.6086961e-03 -4.8957514e-03  3.9185942e-03  4.9220170e-03\n",
            "  2.3926091e-03 -2.8188038e-03  2.8491246e-03 -8.2562361e-03\n",
            " -2.7655398e-03 -2.5911583e-03  7.2490061e-03 -3.4634031e-03\n",
            " -6.5997029e-03  4.3404270e-03 -4.7448516e-04 -3.5975564e-03\n",
            "  6.8824720e-03  3.8723124e-03 -3.9002013e-03  7.7188847e-04\n",
            "  9.1435025e-03  7.7546560e-03  6.3618720e-03  4.6673026e-03\n",
            "  2.3844899e-03 -1.8416261e-03 -6.3712932e-03 -3.0181051e-04\n",
            " -1.5653884e-03 -5.7228567e-04 -6.2628710e-03  7.4340473e-03\n",
            " -6.5914928e-03 -7.2392775e-03 -2.7571463e-03 -1.5154004e-03\n",
            " -7.6357173e-03  6.9824100e-04 -5.3261113e-03 -1.2755442e-03\n",
            " -7.3651113e-03  1.9605684e-03  3.2731986e-03 -2.3138524e-05\n",
            " -5.4483581e-03 -1.7260861e-03  7.0849168e-03  3.7362587e-03\n",
            " -8.8810492e-03 -3.4135508e-03  2.3541022e-03  2.1380198e-03\n",
            " -9.4640078e-03  4.5711659e-03 -8.6569972e-03 -7.3870681e-03\n",
            "  3.4831120e-03 -3.4709584e-03  3.5644709e-03  8.8940905e-03\n",
            " -3.5743224e-03  9.3204249e-03  1.7110384e-03  9.8477742e-03\n",
            "  5.7050432e-03 -9.1494834e-03 -3.3277308e-03  6.5301750e-03\n",
            "  5.6027793e-03  8.7055154e-03  6.9261026e-03  8.0388878e-03\n",
            " -9.8230084e-03  4.2988253e-03 -5.0300765e-03  3.5123860e-03\n",
            "  6.0566878e-03  4.3921317e-03  7.5123594e-03  1.4977157e-03\n",
            " -1.2649416e-03  5.7684006e-03 -5.6395675e-03  3.8591625e-05\n",
            "  9.4565870e-03 -5.4812501e-03  3.8142789e-03 -8.1130210e-03]\n",
            "Word: eat, Embedding: [ 9.7702928e-03  8.1651136e-03  1.2809718e-03  5.0975787e-03\n",
            "  1.4081288e-03 -6.4551616e-03 -1.4280510e-03  6.4491653e-03\n",
            " -4.6173059e-03 -3.9930656e-03  4.9244044e-03  2.7130984e-03\n",
            " -1.8479753e-03 -2.8769434e-03  6.0107317e-03 -5.7167388e-03\n",
            " -3.2367026e-03 -6.4878250e-03 -4.2346325e-03 -8.5809948e-03\n",
            " -4.4697891e-03 -8.5112294e-03  1.4037776e-03 -8.6181965e-03\n",
            " -9.9166557e-03 -8.2016252e-03 -6.7726658e-03  6.6805850e-03\n",
            "  3.7845564e-03  3.5616636e-04 -2.9579818e-03 -7.4283206e-03\n",
            "  5.3341867e-04  4.9989222e-04  1.9561886e-04  8.5259555e-04\n",
            "  7.8633073e-04 -6.8160298e-05 -8.0070542e-03 -5.8702733e-03\n",
            " -8.3829118e-03 -1.3120425e-03  1.8206370e-03  7.4171280e-03\n",
            " -1.9634271e-03 -2.3252917e-03  9.4871549e-03  7.9704521e-05\n",
            " -2.4045217e-03  8.6048469e-03  2.6870037e-03 -5.3439722e-03\n",
            "  6.5881060e-03  4.5101536e-03 -7.0544672e-03 -3.2317400e-04\n",
            "  8.3448651e-04  5.7473574e-03 -1.7176545e-03 -2.8065301e-03\n",
            "  1.7484308e-03  8.4717153e-04  1.1928272e-03 -2.6342822e-03\n",
            " -5.9857843e-03  7.3229838e-03  7.5873756e-03  8.2963575e-03\n",
            " -8.5988473e-03  2.6364254e-03 -3.5599626e-03  9.6204039e-03\n",
            "  2.9037679e-03  4.6411133e-03  2.3856151e-03  6.6084778e-03\n",
            " -5.7432903e-03  7.8944126e-03 -2.4109220e-03 -4.5618857e-03\n",
            " -2.0609903e-03  9.7335577e-03 -6.8565905e-03 -2.1917201e-03\n",
            "  7.0009995e-03 -5.5749417e-05 -6.2949671e-03 -6.3935257e-03\n",
            "  8.9403950e-03  6.4295758e-03  4.7735930e-03 -3.2620477e-03\n",
            " -9.2676198e-03  3.7868882e-03  7.1605504e-03 -5.6328895e-03\n",
            " -7.8650126e-03 -2.9727400e-03 -4.9318983e-03 -2.3151112e-03]\n",
            "Word: to, Embedding: [-1.9442164e-03 -5.2675214e-03  9.4471136e-03 -9.2987325e-03\n",
            "  4.5039477e-03  5.4041781e-03 -1.4092624e-03  9.0070926e-03\n",
            "  9.8853596e-03 -5.4750429e-03 -6.0210000e-03 -6.7469729e-03\n",
            " -7.8948820e-03 -3.0479168e-03 -5.5940272e-03 -8.3446801e-03\n",
            "  7.8290224e-04  2.9946566e-03  6.4147436e-03 -2.6289499e-03\n",
            " -4.4534765e-03  1.2495709e-03  3.9146186e-04  8.1169987e-03\n",
            "  1.8280029e-04  7.2315861e-03 -8.2645155e-03  8.4335366e-03\n",
            " -1.8889094e-03  8.7011540e-03 -7.6168370e-03  1.7963862e-03\n",
            "  1.0564864e-03  4.6005251e-05 -5.1032533e-03 -9.2476979e-03\n",
            " -7.2642174e-03 -7.9511739e-03  1.9137275e-03  4.7846674e-04\n",
            " -1.8131376e-03  7.1201660e-03 -2.4756920e-03 -1.3473093e-03\n",
            " -8.9005642e-03 -9.9254129e-03  8.9493981e-03 -5.7539381e-03\n",
            " -6.3729975e-03  5.1994072e-03  6.6699935e-03 -6.8316413e-03\n",
            "  9.5975993e-04 -6.0084737e-03  1.6473436e-03 -4.2892788e-03\n",
            " -3.4407973e-03  2.1856665e-03  8.6615775e-03  6.7281104e-03\n",
            " -9.6770572e-03 -5.6221043e-03  7.8803329e-03  1.9893574e-03\n",
            " -4.2560520e-03  5.9881213e-04  9.5209610e-03 -1.1027169e-03\n",
            " -9.4246380e-03  1.6084099e-03  6.2323548e-03  6.2823701e-03\n",
            "  4.0916502e-03 -5.6502391e-03 -3.7069322e-04 -5.5317880e-05\n",
            "  4.5717955e-03 -8.0415895e-03 -8.0183093e-03  2.6475071e-04\n",
            " -8.6082993e-03  5.8201565e-03 -4.1781188e-04  9.9711772e-03\n",
            " -5.3439774e-03 -4.8613906e-04  7.7567734e-03 -4.0679323e-03\n",
            " -5.0159004e-03  1.5900708e-03  2.6506938e-03 -2.5649595e-03\n",
            "  6.4475285e-03 -7.6599526e-03  3.3935606e-03  4.8997044e-04\n",
            "  8.7321829e-03  5.9827138e-03  6.8153618e-03  7.8225443e-03]\n",
            "Word: like, Embedding: [-0.00950012  0.00956222 -0.00777076 -0.00264551 -0.00490641 -0.0049667\n",
            " -0.00802359 -0.00778358 -0.00455321 -0.00127536 -0.00510299  0.00614054\n",
            " -0.00951662 -0.0053071   0.00943715  0.00699133  0.00767582  0.00423474\n",
            "  0.00050709 -0.00598114  0.00601878  0.00263503  0.00769943  0.00639384\n",
            "  0.00794257  0.00865741 -0.00989575 -0.0067557   0.00133757  0.0064403\n",
            "  0.00737382  0.00551698  0.00766163 -0.00512557  0.00658441 -0.00410837\n",
            " -0.00905534  0.00914168  0.0013314  -0.00275968 -0.00247784 -0.00422048\n",
            "  0.00481234  0.00440022 -0.00265336 -0.00734188 -0.00356585 -0.00033661\n",
            "  0.00609589 -0.00283734 -0.00012089  0.00087973 -0.00709565  0.002065\n",
            " -0.00143242  0.00280215  0.00484222 -0.00135202 -0.00278014  0.00773865\n",
            "  0.0050456   0.00671352  0.00451564  0.00866716  0.00747497 -0.00108189\n",
            "  0.00874764  0.00460172  0.00544063 -0.00138608 -0.00204132 -0.00442435\n",
            " -0.0085152   0.00303773  0.00888319  0.00891974 -0.00194235  0.00608616\n",
            "  0.00377972 -0.00429597  0.00204292 -0.00543789  0.00820889  0.00543291\n",
            "  0.00318443  0.00410257  0.00865715  0.00727203 -0.00083347 -0.00707277\n",
            "  0.00838047  0.00723358  0.00173047 -0.00134749 -0.00589009 -0.00453309\n",
            "  0.00864797 -0.00313511 -0.00633882  0.00987008]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"I like to eat pizza.\",\n",
        "    \"Pizza is delicious.\",\n",
        "    \"I enjoy eating pizza with friends.\",\n",
        "    \"Friends are important.\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Get word embeddings\n",
        "word_embeddings = model.wv\n",
        "\n",
        "# Print word embeddings\n",
        "for word in word_embeddings.key_to_index:\n",
        "    print(f\"Word: {word}, Embedding: {word_embeddings[word]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wDjmSXVU5z9",
        "outputId": "cc8abf3b-c4ba-46b6-ea58-4acdbcbbabea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: running, Lemmatized: running\n",
            "Original: ate, Lemmatized: ate\n",
            "Original: better, Lemmatized: better\n",
            "Original: dogs, Lemmatized: dog\n",
            "Original: went, Lemmatized: went\n",
            "Original: children, Lemmatized: child\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example words to lemmatize\n",
        "words = [\"running\", \"ate\", \"better\", \"dogs\", \"went\", \"children\"]\n",
        "\n",
        "# Lemmatize the words\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Print the original words and their lemmatized forms\n",
        "for word, lemma in zip(words, lemmatized_words):\n",
        "    print(f\"Original: {word}, Lemmatized: {lemma}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XMbmPl8VHob"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPYsoBfcnfPp",
        "outputId": "91744c55-9a61-4c78-95c0-d3887bf4661a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy-langdetect\n",
            "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from spacy-langdetect) (7.4.4)\n",
            "Collecting langdetect==1.0.7 (from spacy-langdetect)\n",
            "  Downloading langdetect-1.0.7.zip (998 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m998.1/998.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect==1.0.7->spacy-langdetect) (1.16.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect) (24.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect) (1.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect) (2.0.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993414 sha256=17fd440161121f413b03d8cb23548f1e1224dfb8e708241f23d8b2b91207cc41\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/f1/e4/8b73f7a0421b132755956892d29b1e764d3e0857a6e92e32fe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect, spacy-langdetect\n",
            "Successfully installed langdetect-1.0.7 spacy-langdetect-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy-langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDnmLNVRngLQ",
        "outputId": "0a45358f-737f-47fb-c84e-81b9eae2e2d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language (English): en\n",
            "Language (French): fr\n",
            "Language (Japanese): ja\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy_langdetect import LanguageDetector\n",
        "\n",
        "# Load the multilingual pipeline\n",
        "nlp = spacy.blank(\"xx\")  # Load a blank model, language will be detected automatically\n",
        "\n",
        "# Add the sentence tokenizer component to the pipeline\n",
        "sentencizer = nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "# Define a factory function for the LanguageDetector component if it's not already defined\n",
        "if \"language_detector\" not in nlp.factories:\n",
        "    @spacy.Language.factory(\"language_detector\")\n",
        "    def language_detector_factory(nlp, name):\n",
        "        return LanguageDetector()\n",
        "\n",
        "# Add the language detection component to the pipeline\n",
        "nlp.add_pipe(\"language_detector\", last=True)\n",
        "\n",
        "# Function to detect language\n",
        "def detect_language(text):\n",
        "    doc = nlp(text)\n",
        "    return doc._.language[\"language\"]\n",
        "\n",
        "# Example usage:\n",
        "sample_text_english = \"Hello, how are you?\"\n",
        "sample_text_french = \"Bonjour, comment ça va?\"\n",
        "sample_text_japanese = \"こんにちは、元気ですか？\"\n",
        "\n",
        "print(\"Language (English):\", detect_language(sample_text_english))\n",
        "print(\"Language (French):\", detect_language(sample_text_french))\n",
        "print(\"Language (Japanese):\", detect_language(sample_text_japanese))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21i1VhllnYWd"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
